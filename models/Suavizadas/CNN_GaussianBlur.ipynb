{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de género con imágenes de manos — GaussianBlur\n",
    "\n",
    "## Este notebook implementa un modelo basado en redes neuronales convolucionales (CNN), inspirado en una arquitectura tipo ResNet, para la clasificación de género a partir de imágenes de manos. \n",
    "## Las imágenes se suavizan mediante un filtro de desenfoque gaussiano y se normalizan usando Z-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importación de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración inicial del entorno y parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE    = (224, 224)\n",
    "BATCH_SIZE  = 32\n",
    "KERNEL_SIZE = 15\n",
    "\n",
    "csv_path   = \"../../dataset/HandInfo.csv\"\n",
    "image_root = \"../../dataset/image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de etiquetas y codificación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df[\"label\"] = df[\"gender\"].map({\"male\": 0, \"female\": 1})\n",
    "NUM_CLASSES = df[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Función de Normalización Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalization(image: np.ndarray) -> np.ndarray:\n",
    "    mean = image.mean()\n",
    "    std  = image.std()\n",
    "    return (image - mean) / (std + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento de imágenes con Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path_bytes: bytes) -> np.ndarray:\n",
    "    path = path_bytes.decode(\"utf-8\")\n",
    "    img  = cv2.imread(path)\n",
    "    if img is None:\n",
    "        return np.zeros((*IMG_SIZE, 3), dtype=np.float32)\n",
    "    img = cv2.resize(img, IMG_SIZE).astype(np.float32)\n",
    "    img = cv2.GaussianBlur(img, (KERNEL_SIZE, KERNEL_SIZE), 0)\n",
    "    return z_score_normalization(img)\n",
    "\n",
    "def tf_preprocess(path, label):\n",
    "    img = tf.numpy_function(preprocess_image, [path], tf.float32)\n",
    "    img.set_shape((*IMG_SIZE, 3))\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construcción de los datasets para entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(split_name: str):\n",
    "    folder = os.path.join(image_root, split_name)\n",
    "    files  = set(os.listdir(folder))\n",
    "    subdf  = df[df[\"imageName\"].isin(files)].sort_values(\"imageName\")\n",
    "    paths  = [os.path.join(folder, fn) for fn in subdf[\"imageName\"]]\n",
    "    labels = subdf[\"label\"].values\n",
    "    ds     = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    ds     = ds.map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if split_name == \"train\":\n",
    "        ds = ds.shuffle(1000)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = create_dataset(\"train\")\n",
    "val_ds   = create_dataset(\"val\")\n",
    "test_ds  = create_dataset(\"test\")\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")\n",
    "print(f\"Test samples: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Definición de bloques residuales y construcción de una red ResNet simplificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, downsample=False):\n",
    "    identity = x\n",
    "    stride   = 2 if downsample else 1\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, strides=stride, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or identity.shape[-1] != filters:\n",
    "        identity = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(identity)\n",
    "        identity = layers.BatchNormalization()(identity)\n",
    "\n",
    "    x = layers.Add()([x, identity])\n",
    "    return layers.ReLU()(x)\n",
    "\n",
    "def build_resnet(input_shape=(224,224,3), num_classes=2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x      = layers.Conv2D(64, 7, strides=2, padding=\"same\", use_bias=False)(inputs)\n",
    "    x      = layers.BatchNormalization()(x)\n",
    "    x      = layers.ReLU()(x)\n",
    "    x      = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    for _ in range(2):\n",
    "        x = residual_block(x, 64)\n",
    "    for i in range(3):\n",
    "        x = residual_block(x, 128, downsample=(i==0))\n",
    "    for i in range(4):\n",
    "        x = residual_block(x, 256, downsample=(i==0))\n",
    "    x      = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs= layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.09\n",
    "optimizer  = SGD(learning_rate=initial_lr, momentum=0.9, decay=0.001)\n",
    "model      = build_resnet(num_classes=NUM_CLASSES)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Callback personalizado para evaluar sobre test al final de cada época"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_dataset):\n",
    "        super().__init__()\n",
    "        self.test_ds = test_dataset\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss, acc = self.model.evaluate(self.test_ds, verbose=0)\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_accuracies.append(acc)\n",
    "        print(f\"  → Test loss: {loss:.4f} — Test acc: {acc:.4f}\")\n",
    "\n",
    "test_cb = TestMetricsCallback(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Scheduler para modificar el learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch == 3:  # reduce a epoch 4\n",
    "        return lr * 0.7\n",
    "    return lr\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[lr_cb, test_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gaussian_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualización de la evolución de accuracy y pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
    "train_acc = history.history[\"accuracy\"]\n",
    "test_acc  = test_cb.test_accuracies\n",
    "train_loss= history.history[\"loss\"]\n",
    "test_loss = test_cb.test_losses\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(epochs, train_acc, marker='o', color='blue', label='Train')\n",
    "ax1.plot(epochs, test_acc,  marker='o', color='red', label='Test')\n",
    "ax1.set_title('Accuracy over epochs')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_xticks(epochs)\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(epochs, train_loss, marker='o', color='blue', label='Train')\n",
    "ax2.plot(epochs, test_loss,  marker='o', color='red', label='Test')\n",
    "ax2.set_title('Loss over epochs')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
