{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617bd839",
   "metadata": {},
   "source": [
    "## Clasificación de género con imágenes de manos — Filtro Laplaciano\n",
    "\n",
    "# En este notebook se implementa una red neuronal convolucional tipo ResNet-light para clasificar el género a partir de imágenes de manos. Se utiliza un preprocesamiento basado en el operador Laplaciano, que resalta los contornos y detalles locales de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b97ef",
   "metadata": {},
   "source": [
    "## 1. Importación de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63972d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3e5f4",
   "metadata": {},
   "source": [
    "## 2. Configuración de parámetros generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE    = (224, 224)\n",
    "BATCH_SIZE  = 32\n",
    "\n",
    "csv_path   = \"../../dataset/HandInfo.csv\"\n",
    "image_root = \"../../dataset/image\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed4e9a",
   "metadata": {},
   "source": [
    "## 3. Carga de etiquetas y mapeo de género"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df[\"label\"] = df[\"gender\"].map({\"male\": 0, \"female\": 1})\n",
    "NUM_CLASSES = df[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c572bc5",
   "metadata": {},
   "source": [
    "## 4. Función de normalización Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ffb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalization(image: np.ndarray) -> np.ndarray:\n",
    "    mean = image.mean()\n",
    "    std  = image.std()\n",
    "    return (image - mean) / (std + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac14d7",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento con el filtro Laplaciano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path_bytes: bytes) -> np.ndarray:\n",
    "    path = path_bytes.decode(\"utf-8\")\n",
    "    img  = cv2.imread(path)\n",
    "    if img is None:\n",
    "        return np.zeros((*IMG_SIZE, 3), dtype=np.float32)\n",
    "    img = cv2.resize(img, IMG_SIZE).astype(np.float32)\n",
    "    img = cv2.Laplacian(img, cv2.CV_32F)\n",
    "    return z_score_normalization(img)\n",
    "\n",
    "def tf_preprocess(path, label):\n",
    "    img = tf.numpy_function(preprocess_image, [path], tf.float32)\n",
    "    img.set_shape((*IMG_SIZE, 3))\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7523bf2",
   "metadata": {},
   "source": [
    "## 6. Creación de datasets (entrenamiento, validación, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(split_name: str):\n",
    "    folder = os.path.join(image_root, split_name)\n",
    "    files  = set(os.listdir(folder))\n",
    "    subdf  = df[df[\"imageName\"].isin(files)].sort_values(\"imageName\")\n",
    "    paths  = [os.path.join(folder, fn) for fn in subdf[\"imageName\"]]\n",
    "    labels = subdf[\"label\"].values\n",
    "    ds     = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    ds     = ds.map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if split_name == \"train\":\n",
    "        ds = ds.shuffle(1000)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = create_dataset(\"train\")\n",
    "val_ds   = create_dataset(\"val\")\n",
    "test_ds  = create_dataset(\"test\")\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")\n",
    "print(f\"Test samples: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bec29",
   "metadata": {},
   "source": [
    "## 7. Definición de bloques residuales y red tipo ResNet-light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, downsample=False):\n",
    "    identity = x\n",
    "    stride   = 2 if downsample else 1\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, strides=stride, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or identity.shape[-1] != filters:\n",
    "        identity = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(identity)\n",
    "        identity = layers.BatchNormalization()(identity)\n",
    "\n",
    "    x = layers.Add()([x, identity])\n",
    "    return layers.ReLU()(x)\n",
    "\n",
    "def build_resnet(input_shape=(224,224,3), num_classes=2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x      = layers.Conv2D(64, 7, strides=2, padding=\"same\", use_bias=False)(inputs)\n",
    "    x      = layers.BatchNormalization()(x)\n",
    "    x      = layers.ReLU()(x)\n",
    "    x      = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    for _ in range(2):\n",
    "        x = residual_block(x, 64)\n",
    "    for i in range(3):\n",
    "        x = residual_block(x, 128, downsample=(i==0))\n",
    "    for i in range(4):\n",
    "        x = residual_block(x, 256, downsample=(i==0))\n",
    "    x      = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs= layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63166b45",
   "metadata": {},
   "source": [
    "## 8. Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d828cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.09\n",
    "optimizer  = SGD(learning_rate=initial_lr, momentum=0.9, decay=0.001)\n",
    "model      = build_resnet(num_classes=NUM_CLASSES)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198ec5b",
   "metadata": {},
   "source": [
    "## 9. Callback personalizado para evaluar sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05482fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_dataset):\n",
    "        super().__init__()\n",
    "        self.test_ds = test_dataset\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss, acc = self.model.evaluate(self.test_ds, verbose=0)\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_accuracies.append(acc)\n",
    "        print(f\"  → Test loss: {loss:.4f} — Test acc: {acc:.4f}\")\n",
    "\n",
    "test_cb = TestMetricsCallback(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82595c4a",
   "metadata": {},
   "source": [
    "## 10. Scheduler para modificar el learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch == 3:  # reduce a epoch 4\n",
    "        return lr * 0.7\n",
    "    return lr\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d17b80",
   "metadata": {},
   "source": [
    "## 11. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ae635",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[lr_cb, test_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4ad93",
   "metadata": {},
   "source": [
    "## 12. Guardado del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"laplacian_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd61350",
   "metadata": {},
   "source": [
    "## 13. Visualización de métricas: precisión y pérdida por época"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
    "train_acc = history.history[\"accuracy\"]\n",
    "test_acc  = test_cb.test_accuracies\n",
    "train_loss= history.history[\"loss\"]\n",
    "test_loss = test_cb.test_losses\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(epochs, train_acc, marker='o', color='blue', label='Train')\n",
    "ax1.plot(epochs, test_acc,  marker='o', color='red', label='Test')\n",
    "ax1.set_title('Accuracy over epochs')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_xticks(epochs)\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(epochs, train_loss, marker='o', color='blue', label='Train')\n",
    "ax2.plot(epochs, test_loss,  marker='o', color='red', label='Test')\n",
    "ax2.set_title('Loss over epochs')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3d4f8",
   "metadata": {},
   "source": [
    "## 🔄 Preprocesamiento con Laplacian (actualizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbdc40",
   "metadata": {},
   "source": [
    "# Aplicamos el filtro Laplaciano (con ksize=3) para extraer detalles de alta frecuencia y normalizamos con Z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path_bytes: bytes) -> np.ndarray:\n",
    "    path = path_bytes.decode(\"utf-8\")\n",
    "    img  = cv2.imread(path).astype(np.float32)\n",
    "    if img is None:\n",
    "        return np.zeros((*IMG_SIZE, 3), dtype=np.float32)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    lap = cv2.Laplacian(img, cv2.CV_32F, ksize=3)\n",
    "    return z_score_normalization(lap)\n",
    "\n",
    "def tf_preprocess(path, label):\n",
    "    img = tf.numpy_function(preprocess_image, [path], tf.float32)\n",
    "    img.set_shape((*IMG_SIZE, 3))\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fdcde",
   "metadata": {},
   "source": [
    "## Construimos los datasets nuevamente con el nuevo preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(\"train\")\n",
    "val_ds   = create_dataset(\"val\")\n",
    "test_ds  = create_dataset(\"test\")\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")\n",
    "print(f\"Test samples: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5abeb",
   "metadata": {},
   "source": [
    "## Añadimos regularización L2 a las capas convolucionales y Dropout antes de la capa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd10fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def residual_block(x, filters, downsample=False):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    identity = x\n",
    "    stride   = 2 if downsample else 1\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, strides=stride, padding=\"same\",\n",
    "                      use_bias=False, kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, 3, strides=1, padding=\"same\",\n",
    "                      use_bias=False, kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or identity.shape[-1] != filters:\n",
    "        identity = layers.Conv2D(filters, 1, strides=stride,\n",
    "                                 use_bias=False, kernel_regularizer=reg)(identity)\n",
    "        identity = layers.BatchNormalization()(identity)\n",
    "\n",
    "    x = layers.Add()([x, identity])\n",
    "    return layers.ReLU()(x)\n",
    "\n",
    "def build_resnet(input_shape=(224,224,3), num_classes=2):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x      = layers.Conv2D(64, 7, strides=2, padding=\"same\",\n",
    "                           use_bias=False, kernel_regularizer=reg)(inputs)\n",
    "    x      = layers.BatchNormalization()(x)\n",
    "    x      = layers.ReLU()(x)\n",
    "    x      = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    for _ in range(2):\n",
    "        x = residual_block(x, 64)\n",
    "    for i in range(3):\n",
    "        x = residual_block(x, 128, downsample=(i==0))\n",
    "    for i in range(4):\n",
    "        x = residual_block(x, 256, downsample=(i==0))\n",
    "\n",
    "    x      = layers.GlobalAveragePooling2D()(x)\n",
    "    x      = layers.Dropout(0.5)(x)\n",
    "    outputs= layers.Dense(num_classes, activation=\"softmax\",\n",
    "                          kernel_regularizer=reg)(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851e75b",
   "metadata": {},
   "source": [
    "## Usamos una tasa de aprendizaje inicial más baja, adecuada para detalles finos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.03\n",
    "optimizer  = SGD(learning_rate=initial_lr, momentum=0.9)\n",
    "model      = build_resnet(num_classes=NUM_CLASSES)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939502f",
   "metadata": {},
   "source": [
    "## Scheduler para disminuir el LR en la 4ª época"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da82a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch == 3:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feb707",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2308d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[lr_cb, test_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdd47a",
   "metadata": {},
   "source": [
    "## Guardado del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"laplacian_model_regularized.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0ff49",
   "metadata": {},
   "source": [
    "## Visualización de métricas: precisión y pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e841a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
    "train_acc = history.history[\"accuracy\"]\n",
    "test_acc  = test_cb.test_accuracies\n",
    "train_loss= history.history[\"loss\"]\n",
    "test_loss = test_cb.test_losses\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(epochs, train_acc, marker='o', color='blue', label='Train')\n",
    "ax1.plot(epochs, test_acc,  marker='s', color='red', label='Test')\n",
    "ax1.set_title('Accuracy over epochs')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_xticks(epochs)\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(epochs, train_loss, marker='o', color='blue', label='Train')\n",
    "ax2.plot(epochs, test_loss,  marker='s', color='red', label='Test')\n",
    "ax2.set_title('Loss over epochs')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
